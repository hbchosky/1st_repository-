{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba1d8f0",
   "metadata": {},
   "source": [
    "# 뉴스카테고리 다중분류[프로젝트]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e5c83",
   "metadata": {},
   "source": [
    "![모델결과](./Reuters_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed304906",
   "metadata": {},
   "source": [
    "### 주어진 단어 수 수준에서 각 모델의 정확도와 F1 점수 \n",
    "- ML모델인 선형 SVM이 모든 단어 수에서 일관되게 가장 높은 정확도와 F1 점수를 보임\n",
    "- XGBoost와 KNN를 포함한 ML 모델이 상대적으로 우수한 성능을 보임 \n",
    "- DL모델인 CNN+LSTM, LSTM, RNN과 같은 모델은 일반적으로 점수가 낮음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83797ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Word Count Linear SVM (Acc / F1) XGBoost (Acc / F1)   KNN (Acc / F1)  \\\n",
      "0  10000 words       0.8299 / 0.6808    0.7925 / 0.6387  0.7894 / 0.5903   \n",
      "1   5000 words       0.8290 / 0.6813    0.7983 / 0.6396  0.7827 / 0.5818   \n",
      "2    all words       0.8295 / 0.6887    0.7979 / 0.6546  0.7720 / 0.5769   \n",
      "\n",
      "  Logistic Regression (Acc / F1) Decision Tree (Acc / F1)  \\\n",
      "0                0.7956 / 0.4721          0.6941 / 0.4580   \n",
      "1                0.7979 / 0.4814          0.6901 / 0.4548   \n",
      "2                0.7916 / 0.4514          0.7026 / 0.4577   \n",
      "\n",
      "  Multinomial NB (Acc / F1) Random Forest (Acc / F1)   CNN (Acc / F1)  \\\n",
      "0           0.7711 / 0.4386          0.7560 / 0.4367  0.7217 / 0.2968   \n",
      "1           0.7774 / 0.5096          0.7671 / 0.4561  0.7244 / 0.3085   \n",
      "2           0.7226 / 0.2513          0.7400 / 0.4302  0.7177 / 0.3027   \n",
      "\n",
      "  CNN+LSTM (Acc / F1)  LSTM (Acc / F1)   RNN (Acc / F1)  \n",
      "0     0.6447 / 0.1153  0.6024 / 0.0734  0.4795 / 0.0311  \n",
      "1     0.6901 / 0.1639  0.6051 / 0.0786  0.4960 / 0.0311  \n",
      "2     0.6336 / 0.1150  0.6616 / 0.1533  0.4604 / 0.0262  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"./Reuters_classification_results_h.csv\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32744130",
   "metadata": {},
   "source": [
    " text classification on the Reuters dataset using:\n",
    " - DTM, TF-IDF representations\n",
    " - Machine Learning models (Logistic Regression, SVM, etc.)\n",
    " - Word2Vec embedding\n",
    " - Deep Learning models (LSTM, CNN, CNN+LSTM, RNN)#\n",
    " - Evaluation with accuracy, F1, and confusion matrix visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3940e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/hb/miniconda3/envs/moduenv/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "%pip install gensim\n",
    "%pip install xgboost\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, SimpleRNN, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8869ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## 2. Load Reuters Dataset\n",
    "\n",
    "# Load data and word index\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None)\n",
    "#(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2) #5000\n",
    "word_index = reuters.get_word_index()\n",
    "index_word = {v + 3: k for k, v in word_index.items()}\n",
    "for k, v in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")): # Add special tokens at the beginning\n",
    "  index_word[k] = v\n",
    "\n",
    "# Decode to raw text for vectorization\n",
    "X_train_text = [' '.join([index_word.get(i, '?') for i in seq]) for seq in X_train]\n",
    "X_test_text = [' '.join([index_word.get(i, '?') for i in seq]) for seq in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2307c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. ML: Vectorize Text \n",
    "# DTM Count Vectorizer\n",
    "dtmvector = CountVectorizer() #Naive Bayes prefers DTM \n",
    "X_train_dtm = dtmvector.fit_transform(X_train_text)\n",
    "X_test_dtm = dtmvector.transform(X_test_text)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=None)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dad10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## 4. ML Models Setup\n",
    "ml_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    #\"Extra Trees\": ExtraTreesClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "\n",
    "ml_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "941398e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. Confusion Matrix Plot Function\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_norm, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3d39d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.7916, F1 Score: 0.4514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.65      0.83      0.73       105\n",
      "           2       0.92      0.60      0.73        20\n",
      "           3       0.92      0.94      0.93       813\n",
      "           4       0.71      0.93      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.79      0.85        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.75      0.63      0.69        38\n",
      "           9       0.96      0.88      0.92        25\n",
      "          10       0.96      0.77      0.85        30\n",
      "          11       0.61      0.81      0.69        83\n",
      "          12       1.00      0.31      0.47        13\n",
      "          13       0.68      0.62      0.65        37\n",
      "          14       1.00      0.50      0.67         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.71      0.78      0.74        99\n",
      "          17       1.00      0.08      0.15        12\n",
      "          18       0.65      0.65      0.65        20\n",
      "          19       0.68      0.74      0.71       133\n",
      "          20       0.77      0.47      0.58        70\n",
      "          21       0.70      0.78      0.74        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.50      0.08      0.14        12\n",
      "          24       0.75      0.16      0.26        19\n",
      "          25       0.95      0.61      0.75        31\n",
      "          26       1.00      0.25      0.40         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.60      0.25      0.35        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.70      0.82        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       1.00      0.18      0.31        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.33      0.50         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.79      2246\n",
      "   macro avg       0.60      0.40      0.45      2246\n",
      "weighted avg       0.78      0.79      0.77      2246\n",
      "\n",
      "\n",
      "Training MultinomialNB...\n",
      "MultinomialNB - Accuracy: 0.7226, F1 Score: 0.2513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.42      0.56        12\n",
      "           1       0.46      0.83      0.59       105\n",
      "           2       1.00      0.05      0.10        20\n",
      "           3       0.93      0.88      0.90       813\n",
      "           4       0.73      0.92      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.21      0.35        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.71      0.39      0.51        38\n",
      "           9       0.94      0.64      0.76        25\n",
      "          10       1.00      0.30      0.46        30\n",
      "          11       0.42      0.82      0.56        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.56      0.38      0.45        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.51      0.81      0.63        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.67      0.30      0.41        20\n",
      "          19       0.52      0.80      0.63       133\n",
      "          20       0.68      0.40      0.50        70\n",
      "          21       0.74      0.63      0.68        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.19      0.32        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       1.00      0.08      0.15        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       1.00      0.14      0.25         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       1.00      0.18      0.31        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       1.00      0.12      0.22         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72      2246\n",
      "   macro avg       0.41      0.23      0.25      2246\n",
      "weighted avg       0.72      0.72      0.69      2246\n",
      "\n",
      "\n",
      "Training Linear SVM...\n",
      "Linear SVM - Accuracy: 0.8295, F1 Score: 0.6887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.73      0.80      0.76       105\n",
      "           2       0.84      0.80      0.82        20\n",
      "           3       0.93      0.94      0.94       813\n",
      "           4       0.81      0.89      0.85       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.93      0.93      0.93        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.69      0.71      0.70        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.90      0.90      0.90        30\n",
      "          11       0.66      0.76      0.70        83\n",
      "          12       0.88      0.54      0.67        13\n",
      "          13       0.69      0.65      0.67        37\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       0.67      0.22      0.33         9\n",
      "          16       0.74      0.77      0.75        99\n",
      "          17       0.75      0.50      0.60        12\n",
      "          18       0.81      0.65      0.72        20\n",
      "          19       0.71      0.72      0.71       133\n",
      "          20       0.69      0.54      0.61        70\n",
      "          21       0.71      0.89      0.79        27\n",
      "          22       1.00      0.29      0.44         7\n",
      "          23       0.64      0.75      0.69        12\n",
      "          24       0.67      0.53      0.59        19\n",
      "          25       0.90      0.84      0.87        31\n",
      "          26       0.88      0.88      0.88         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.38      0.30      0.33        10\n",
      "          29       0.80      1.00      0.89         4\n",
      "          30       1.00      0.67      0.80        12\n",
      "          31       1.00      0.69      0.82        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.43      0.27      0.33        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.40      0.57         5\n",
      "          40       1.00      0.20      0.33        10\n",
      "          41       0.75      0.38      0.50         8\n",
      "          42       1.00      0.67      0.80         3\n",
      "          43       0.86      1.00      0.92         6\n",
      "          44       0.67      0.80      0.73         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.83      2246\n",
      "   macro avg       0.80      0.65      0.69      2246\n",
      "weighted avg       0.83      0.83      0.82      2246\n",
      "\n",
      "\n",
      "Training KNN...\n",
      "KNN - Accuracy: 0.7720, F1 Score: 0.5769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61        12\n",
      "           1       0.52      0.79      0.62       105\n",
      "           2       0.65      0.55      0.59        20\n",
      "           3       0.87      0.94      0.91       813\n",
      "           4       0.84      0.76      0.80       474\n",
      "           5       1.00      0.20      0.33         5\n",
      "           6       0.79      0.79      0.79        14\n",
      "           7       0.60      1.00      0.75         3\n",
      "           8       0.63      0.58      0.60        38\n",
      "           9       0.77      0.92      0.84        25\n",
      "          10       0.79      0.77      0.78        30\n",
      "          11       0.61      0.69      0.65        83\n",
      "          12       0.60      0.46      0.52        13\n",
      "          13       0.61      0.51      0.56        37\n",
      "          14       0.33      0.50      0.40         2\n",
      "          15       0.33      0.11      0.17         9\n",
      "          16       0.71      0.80      0.75        99\n",
      "          17       0.60      0.25      0.35        12\n",
      "          18       0.50      0.65      0.57        20\n",
      "          19       0.66      0.71      0.69       133\n",
      "          20       0.73      0.46      0.56        70\n",
      "          21       0.65      0.63      0.64        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.60      0.50      0.55        12\n",
      "          24       0.50      0.32      0.39        19\n",
      "          25       0.92      0.77      0.84        31\n",
      "          26       0.78      0.88      0.82         8\n",
      "          27       1.00      0.50      0.67         4\n",
      "          28       0.33      0.20      0.25        10\n",
      "          29       1.00      0.50      0.67         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.62      0.38      0.48        13\n",
      "          32       0.70      0.70      0.70        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.83      0.71      0.77         7\n",
      "          35       1.00      0.67      0.80         6\n",
      "          36       0.62      0.73      0.67        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.80      0.50      0.62         8\n",
      "          42       0.75      1.00      0.86         3\n",
      "          43       1.00      0.50      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.77      2246\n",
      "   macro avg       0.65      0.56      0.58      2246\n",
      "weighted avg       0.77      0.77      0.76      2246\n",
      "\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree - Accuracy: 0.7026, F1 Score: 0.4577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73        12\n",
      "           1       0.61      0.69      0.65       105\n",
      "           2       0.60      0.60      0.60        20\n",
      "           3       0.86      0.88      0.87       813\n",
      "           4       0.72      0.73      0.73       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.71      0.71      0.71        14\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.50      0.61      0.55        38\n",
      "           9       0.90      0.76      0.83        25\n",
      "          10       0.75      0.80      0.77        30\n",
      "          11       0.49      0.58      0.53        83\n",
      "          12       0.50      0.46      0.48        13\n",
      "          13       0.41      0.41      0.41        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.62      0.70      0.65        99\n",
      "          17       0.14      0.08      0.11        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.56      0.56      0.56       133\n",
      "          20       0.44      0.34      0.39        70\n",
      "          21       0.44      0.41      0.42        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.15      0.17      0.16        12\n",
      "          24       0.40      0.32      0.35        19\n",
      "          25       0.95      0.65      0.77        31\n",
      "          26       0.78      0.88      0.82         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.43      0.30      0.35        10\n",
      "          29       0.20      0.25      0.22         4\n",
      "          30       0.83      0.42      0.56        12\n",
      "          31       0.50      0.31      0.38        13\n",
      "          32       1.00      0.60      0.75        10\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       0.33      0.29      0.31         7\n",
      "          35       0.67      0.33      0.44         6\n",
      "          36       0.23      0.27      0.25        11\n",
      "          37       0.33      1.00      0.50         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.43      0.30      0.35        10\n",
      "          41       0.50      0.12      0.20         8\n",
      "          42       0.50      0.33      0.40         3\n",
      "          43       0.38      0.50      0.43         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.70      2246\n",
      "   macro avg       0.49      0.46      0.46      2246\n",
      "weighted avg       0.70      0.70      0.70      2246\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.7400, F1 Score: 0.4302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.33      0.47        12\n",
      "           1       0.54      0.75      0.63       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.89      0.91      0.90       813\n",
      "           4       0.63      0.91      0.75       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.43      0.60        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.69      0.63      0.66        38\n",
      "           9       1.00      0.68      0.81        25\n",
      "          10       1.00      0.37      0.54        30\n",
      "          11       0.65      0.75      0.69        83\n",
      "          12       0.50      0.15      0.24        13\n",
      "          13       0.52      0.30      0.38        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.65      0.66      0.65        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.73      0.55      0.63        20\n",
      "          19       0.66      0.76      0.70       133\n",
      "          20       0.72      0.33      0.45        70\n",
      "          21       0.82      0.67      0.73        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.25      0.08      0.12        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       1.00      0.39      0.56        31\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.10      0.17        10\n",
      "          29       0.25      0.25      0.25         4\n",
      "          30       0.75      0.25      0.38        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       1.00      0.30      0.46        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.67      0.29      0.40         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.18      0.27        11\n",
      "          37       1.00      0.50      0.67         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.74      2246\n",
      "   macro avg       0.63      0.37      0.43      2246\n",
      "weighted avg       0.74      0.74      0.71      2246\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost - Accuracy: 0.7979, F1 Score: 0.6546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.75      0.82        12\n",
      "           1       0.74      0.77      0.75       105\n",
      "           2       0.73      0.80      0.76        20\n",
      "           3       0.90      0.92      0.91       813\n",
      "           4       0.78      0.86      0.82       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.71      0.66      0.68        38\n",
      "           9       0.81      0.84      0.82        25\n",
      "          10       0.90      0.87      0.88        30\n",
      "          11       0.70      0.67      0.69        83\n",
      "          12       0.70      0.54      0.61        13\n",
      "          13       0.61      0.54      0.57        37\n",
      "          14       0.40      1.00      0.57         2\n",
      "          15       0.50      0.22      0.31         9\n",
      "          16       0.68      0.74      0.71        99\n",
      "          17       0.75      0.75      0.75        12\n",
      "          18       0.87      0.65      0.74        20\n",
      "          19       0.65      0.71      0.68       133\n",
      "          20       0.62      0.49      0.54        70\n",
      "          21       0.66      0.70      0.68        27\n",
      "          22       1.00      0.14      0.25         7\n",
      "          23       0.64      0.58      0.61        12\n",
      "          24       0.77      0.53      0.62        19\n",
      "          25       0.85      0.74      0.79        31\n",
      "          26       0.86      0.75      0.80         8\n",
      "          27       1.00      0.25      0.40         4\n",
      "          28       0.50      0.30      0.38        10\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       1.00      0.50      0.67        12\n",
      "          31       0.80      0.62      0.70        13\n",
      "          32       1.00      0.90      0.95        10\n",
      "          33       0.80      0.80      0.80         5\n",
      "          34       1.00      0.57      0.73         7\n",
      "          35       1.00      0.50      0.67         6\n",
      "          36       0.50      0.45      0.48        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.50      0.33      0.40         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       1.00      0.30      0.46        10\n",
      "          41       0.83      0.62      0.71         8\n",
      "          42       0.67      0.67      0.67         3\n",
      "          43       0.80      0.67      0.73         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.80      2246\n",
      "   macro avg       0.76      0.62      0.65      2246\n",
      "weighted avg       0.80      0.80      0.79      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Train and Evaluate ML Models\n",
    "\n",
    "for name, model in ml_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    if name == \"MultinomialNB\":\n",
    "        # MultinomialNB works better with DTM\n",
    "        model.fit(X_train_dtm, y_train)\n",
    "        y_pred = model.predict(X_test_dtm)\n",
    "    else:\n",
    "        # Other models use TF-IDF\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(y_test, y_pred, name)\n",
    "\n",
    "    ml_results[name] = (acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abcbe93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Prepare Data for DL\n",
    "\n",
    "maxlen = 300\n",
    "vocab_size = 100000\n",
    "embedding_dim = 100\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=maxlen)\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# ## 8. Train Word2Vec on Full Text\n",
    "\n",
    "sentences = [text.split() for text in X_train_text + X_test_text]\n",
    "w2v_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01d8e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 9. Define DL Models\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        SimpleRNN(64),\n",
    "        Dropout(0.5),\n",
    "        Dense(46, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        LSTM(64),\n",
    "        Dropout(0.5),\n",
    "        Dense(46, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(46, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_cnn_lstm():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.5),\n",
    "        LSTM(64),\n",
    "        Dropout(0.5),\n",
    "        Dense(46, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c218247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n",
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 191ms/step - accuracy: 0.3422 - loss: 2.7957 - val_accuracy: 0.4819 - val_loss: 2.0674\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 142ms/step - accuracy: 0.4864 - loss: 2.0942 - val_accuracy: 0.4864 - val_loss: 1.9835\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 144ms/step - accuracy: 0.4953 - loss: 1.9982 - val_accuracy: 0.5309 - val_loss: 1.8083\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 140ms/step - accuracy: 0.5351 - loss: 1.8700 - val_accuracy: 0.5609 - val_loss: 1.7613\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 197ms/step - accuracy: 0.5530 - loss: 1.7913 - val_accuracy: 0.5008 - val_loss: 1.9916\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - accuracy: 0.5208 - loss: 1.8767 - val_accuracy: 0.5665 - val_loss: 1.6686\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - accuracy: 0.5792 - loss: 1.6645 - val_accuracy: 0.5810 - val_loss: 1.6363\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 225ms/step - accuracy: 0.5973 - loss: 1.6304 - val_accuracy: 0.6077 - val_loss: 1.5489\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 228ms/step - accuracy: 0.6066 - loss: 1.5594 - val_accuracy: 0.5832 - val_loss: 1.6051\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 208ms/step - accuracy: 0.6039 - loss: 1.5847 - val_accuracy: 0.6127 - val_loss: 1.5124\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 221ms/step - accuracy: 0.6320 - loss: 1.4783 - val_accuracy: 0.6183 - val_loss: 1.4869\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 250ms/step - accuracy: 0.6356 - loss: 1.4616 - val_accuracy: 0.6383 - val_loss: 1.4575\n",
      "Epoch 13/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - accuracy: 0.6393 - loss: 1.4248 - val_accuracy: 0.6322 - val_loss: 1.4281\n",
      "Epoch 14/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 176ms/step - accuracy: 0.6602 - loss: 1.3276 - val_accuracy: 0.5198 - val_loss: 1.7496\n",
      "Epoch 15/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 176ms/step - accuracy: 0.6108 - loss: 1.4866 - val_accuracy: 0.6633 - val_loss: 1.4008\n",
      "Epoch 16/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - accuracy: 0.6729 - loss: 1.3205 - val_accuracy: 0.6522 - val_loss: 1.3886\n",
      "Epoch 17/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.6803 - loss: 1.2830 - val_accuracy: 0.6644 - val_loss: 1.3429\n",
      "Epoch 18/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 169ms/step - accuracy: 0.6730 - loss: 1.2831 - val_accuracy: 0.6633 - val_loss: 1.3686\n",
      "Epoch 19/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 152ms/step - accuracy: 0.6947 - loss: 1.2267 - val_accuracy: 0.6728 - val_loss: 1.3428\n",
      "Epoch 20/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.6968 - loss: 1.2052 - val_accuracy: 0.6784 - val_loss: 1.3058\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
      "LSTM - Accuracy: 0.6616, F1 Score: 0.1533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.33      0.64      0.43       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.90      0.91      0.91       813\n",
      "           4       0.76      0.84      0.80       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       1.00      0.07      0.13        14\n",
      "           7       0.33      0.33      0.33         3\n",
      "           8       0.60      0.39      0.48        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.27      0.10      0.15        30\n",
      "          11       0.46      0.67      0.55        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.10      0.08      0.09        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.33      0.68      0.45        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.40      0.50      0.44        20\n",
      "          19       0.48      0.65      0.55       133\n",
      "          20       0.39      0.30      0.34        70\n",
      "          21       0.43      0.22      0.29        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.40      0.06      0.11        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.10      0.14      0.12         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.66      2246\n",
      "   macro avg       0.18      0.16      0.15      2246\n",
      "weighted avg       0.61      0.66      0.63      2246\n",
      "\n",
      "\n",
      "Training CNN...\n",
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 87ms/step - accuracy: 0.3424 - loss: 4.8645 - val_accuracy: 0.5826 - val_loss: 1.7228\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - accuracy: 0.5486 - loss: 1.8939 - val_accuracy: 0.6194 - val_loss: 1.5781\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 90ms/step - accuracy: 0.6174 - loss: 1.6165 - val_accuracy: 0.6494 - val_loss: 1.4900\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.6458 - loss: 1.4900 - val_accuracy: 0.6678 - val_loss: 1.4170\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 106ms/step - accuracy: 0.6621 - loss: 1.4287 - val_accuracy: 0.6867 - val_loss: 1.3553\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 111ms/step - accuracy: 0.6808 - loss: 1.3009 - val_accuracy: 0.6984 - val_loss: 1.3114\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - accuracy: 0.6988 - loss: 1.2387 - val_accuracy: 0.7012 - val_loss: 1.2743\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - accuracy: 0.7143 - loss: 1.1645 - val_accuracy: 0.7190 - val_loss: 1.2408\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 120ms/step - accuracy: 0.7321 - loss: 1.0768 - val_accuracy: 0.7179 - val_loss: 1.2186\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 73ms/step - accuracy: 0.7188 - loss: 1.0930 - val_accuracy: 0.7184 - val_loss: 1.2146\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.7331 - loss: 1.0582 - val_accuracy: 0.7206 - val_loss: 1.1952\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 90ms/step - accuracy: 0.7449 - loss: 1.0085 - val_accuracy: 0.7218 - val_loss: 1.1820\n",
      "Epoch 13/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - accuracy: 0.7556 - loss: 0.9754 - val_accuracy: 0.7212 - val_loss: 1.1886\n",
      "Epoch 14/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 92ms/step - accuracy: 0.7664 - loss: 0.9243 - val_accuracy: 0.7273 - val_loss: 1.1706\n",
      "Epoch 15/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7688 - loss: 0.9057 - val_accuracy: 0.7279 - val_loss: 1.1814\n",
      "Epoch 16/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - accuracy: 0.7700 - loss: 0.8923 - val_accuracy: 0.7312 - val_loss: 1.1617\n",
      "Epoch 17/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - accuracy: 0.7688 - loss: 0.8707 - val_accuracy: 0.7279 - val_loss: 1.1802\n",
      "Epoch 18/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 90ms/step - accuracy: 0.7926 - loss: 0.7951 - val_accuracy: 0.7268 - val_loss: 1.1683\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "CNN - Accuracy: 0.7177, F1 Score: 0.3027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.08      0.14        12\n",
      "           1       0.42      0.61      0.50       105\n",
      "           2       0.80      0.20      0.32        20\n",
      "           3       0.89      0.92      0.91       813\n",
      "           4       0.72      0.89      0.79       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.75      0.21      0.33        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.82      0.47      0.60        38\n",
      "           9       0.78      0.28      0.41        25\n",
      "          10       0.44      0.23      0.30        30\n",
      "          11       0.60      0.63      0.61        83\n",
      "          12       0.80      0.31      0.44        13\n",
      "          13       0.31      0.38      0.34        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.63      0.77      0.69        99\n",
      "          17       0.50      0.08      0.14        12\n",
      "          18       0.55      0.60      0.57        20\n",
      "          19       0.55      0.73      0.63       133\n",
      "          20       0.52      0.47      0.50        70\n",
      "          21       0.38      0.48      0.43        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       1.00      0.05      0.10        19\n",
      "          25       0.80      0.26      0.39        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.67      0.20      0.31        10\n",
      "          29       0.33      0.25      0.29         4\n",
      "          30       0.67      0.33      0.44        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.50      0.10      0.17        10\n",
      "          33       1.00      0.60      0.75         5\n",
      "          34       0.60      0.43      0.50         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.27      0.35        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.72      2246\n",
      "   macro avg       0.46      0.27      0.30      2246\n",
      "weighted avg       0.70      0.72      0.69      2246\n",
      "\n",
      "\n",
      "Training CNN + LSTM...\n",
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 197ms/step - accuracy: 0.3331 - loss: 2.7486 - val_accuracy: 0.4763 - val_loss: 2.0527\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 192ms/step - accuracy: 0.4589 - loss: 2.1578 - val_accuracy: 0.5209 - val_loss: 1.8562\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 227ms/step - accuracy: 0.4960 - loss: 1.9525 - val_accuracy: 0.4953 - val_loss: 1.8994\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 217ms/step - accuracy: 0.5328 - loss: 1.8203 - val_accuracy: 0.5676 - val_loss: 1.6818\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 183ms/step - accuracy: 0.5608 - loss: 1.7589 - val_accuracy: 0.5748 - val_loss: 1.6592\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 176ms/step - accuracy: 0.5827 - loss: 1.6734 - val_accuracy: 0.5993 - val_loss: 1.5886\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - accuracy: 0.5986 - loss: 1.6251 - val_accuracy: 0.5865 - val_loss: 1.5972\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 174ms/step - accuracy: 0.5920 - loss: 1.6574 - val_accuracy: 0.6049 - val_loss: 1.5342\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 199ms/step - accuracy: 0.6027 - loss: 1.5504 - val_accuracy: 0.6132 - val_loss: 1.5422\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 200ms/step - accuracy: 0.6275 - loss: 1.5066 - val_accuracy: 0.6411 - val_loss: 1.4452\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 206ms/step - accuracy: 0.6254 - loss: 1.5015 - val_accuracy: 0.6316 - val_loss: 1.4727\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 208ms/step - accuracy: 0.6594 - loss: 1.3992 - val_accuracy: 0.6155 - val_loss: 1.4931\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step\n",
      "CNN + LSTM - Accuracy: 0.6336, F1 Score: 0.1150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.20      0.70      0.31       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.92      0.91      0.92       813\n",
      "           4       0.77      0.87      0.81       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.60      0.39      0.48        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.34      0.55      0.42        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.20      0.19      0.19        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.29      0.55      0.38        20\n",
      "          19       0.42      0.57      0.48       133\n",
      "          20       0.42      0.31      0.36        70\n",
      "          21       1.00      0.04      0.07        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       1.00      0.03      0.06        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.80      0.80      0.80         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.63      2246\n",
      "   macro avg       0.15      0.13      0.12      2246\n",
      "weighted avg       0.60      0.63      0.60      2246\n",
      "\n",
      "\n",
      "Training RNN...\n",
      "Epoch 1/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - accuracy: 0.2465 - loss: 3.0266 - val_accuracy: 0.3918 - val_loss: 2.3270\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.3709 - loss: 2.5333 - val_accuracy: 0.4062 - val_loss: 2.2854\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.4231 - loss: 2.3635 - val_accuracy: 0.4480 - val_loss: 2.1618\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.3880 - loss: 2.4682 - val_accuracy: 0.3651 - val_loss: 2.3023\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.3769 - loss: 2.3807 - val_accuracy: 0.3745 - val_loss: 2.2865\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "RNN - Accuracy: 0.4604, F1 Score: 0.0262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.00      0.00      0.00       105\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.79      0.75      0.77       813\n",
      "           4       0.29      0.89      0.43       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00        14\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00        38\n",
      "           9       0.00      0.00      0.00        25\n",
      "          10       0.00      0.00      0.00        30\n",
      "          11       0.00      0.00      0.00        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.00      0.00      0.00        20\n",
      "          19       0.00      0.00      0.00       133\n",
      "          20       0.00      0.00      0.00        70\n",
      "          21       0.00      0.00      0.00        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.00      0.00      0.00        12\n",
      "          24       0.00      0.00      0.00        19\n",
      "          25       0.00      0.00      0.00        31\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.00      0.00      0.00        10\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         5\n",
      "          45       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.46      2246\n",
      "   macro avg       0.02      0.04      0.03      2246\n",
      "weighted avg       0.35      0.46      0.37      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## 10. Train and Evaluate DL Models\n",
    "\n",
    "dl_models = {\n",
    "    \"LSTM\": build_lstm,\n",
    "    \"CNN\": build_cnn,\n",
    "    \"CNN + LSTM\": build_cnn_lstm,\n",
    "    \"RNN\": build_rnn\n",
    "}\n",
    "\n",
    "dl_results = {}\n",
    "\n",
    "for name, builder in dl_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = builder()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train_pad, y_train_cat,\n",
    "              validation_split=0.2,\n",
    "              epochs=20,\n",
    "              batch_size=64,\n",
    "              callbacks=[early_stop],\n",
    "              verbose=1)\n",
    "\n",
    "    y_pred_prob = model.predict(X_test_pad)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    #plot_confusion_matrix(y_test, y_pred, name)\n",
    "\n",
    "    dl_results[name] = (acc, f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32300799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Combined Model Performance [all words]===\n",
      "Model                Accuracy   F1 Score  \n",
      "Linear SVM           0.8295     0.6887\n",
      "XGBoost              0.7979     0.6546\n",
      "KNN                  0.7720     0.5769\n",
      "Decision Tree        0.7026     0.4577\n",
      "Logistic Regression  0.7916     0.4514\n",
      "Random Forest        0.7400     0.4302\n",
      "CNN                  0.7177     0.3027\n",
      "MultinomialNB        0.7226     0.2513\n",
      "LSTM                 0.6616     0.1533\n",
      "CNN + LSTM           0.6336     0.1150\n",
      "RNN                  0.4604     0.0262\n"
     ]
    }
   ],
   "source": [
    "# ## 11. Combine and Display Results\n",
    "\n",
    "all_results = {**ml_results, **dl_results}\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "print(\"\\n=== Combined Model Performance [all words]===\")\n",
    "print(\"{:<20} {:<10} {:<10}\".format(\"Model\", \"Accuracy\", \"F1 Score\"))\n",
    "for name, (acc, f1) in sorted_results:\n",
    "    print(\"{:<20} {:.4f}     {:.4f}\".format(name, acc, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e28f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moduenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
